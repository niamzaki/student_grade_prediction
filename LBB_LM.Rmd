---
title: "Linear Regression on Student Grade Prediction"
date: "January 18, 2021"
author: Niam Zaki Zamani
output:
  rmdformats::readthedown:
    gallery: no
    highlight: default
    lightbox: yes
    self_contained: yes
    thumbnails: no
    fig_align: center
  pdf_document:
    toc: yes
    toc_depth: '2'
  word_document:
    toc: yes
    toc_depth: 2

---


## Overview
This time, we will try to do a prediction of students' final grade based on several parameters. By using `Linear Regression`, we want to know the relationship among variables, especially between the Final Score variable or `G3` in this dataset with other variables. we also want to predict the score based on the historical data. The dataset we will use for making prediction is from kaggle that contains 33 attributes for 393 entries. If you interested, the dataset for this project can be accessed [here](https://www.kaggle.com/dipam7/student-grade-prediction).

## Data Preparation

Load required packages.
```{r, message=F, warning=F}
library(car)
library(caret)
library(class)
library(dplyr)
library(GGally)
library(ggplot2)
library(MLmetrics)
library(tidyr)
library(tidyverse)
```

Load the dataset.
```{r}
grade <- read.csv("data_input/student-mat.csv")
```

```{r,echo=FALSE, message=F}
rmarkdown::paged_table(grade, options = list(rows.print = 10))
```


```{r}
glimpse(grade)
```

* `school` - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)
* `sex` - student's sex (binary: 'F' - female or 'M' - male)
* `age` - student's age (numeric: from 15 to 22)
* `address` - student's home address type (binary: 'U' - urban or 'R' - rural)
* `famsize` - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)
* `Pstatus` - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)
* `Medu` - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)
* `Fedu` - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)
* `Mjob` - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
* `Fjob` - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')
* `reason` - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')
* `guardian` - student's guardian (nominal: 'mother', 'father' or 'other')
* `traveltime` - home to school travel time (numeric: 1 - 1 hour)
* `studytime` - weekly study time (numeric: 1 - 10 hours)
* `failures` - number of past class failures (numeric: n if 1<=n<3, else 4)
* `schoolsup` - extra educational support (binary: yes or no)
* `famsup` - family educational support (binary: yes or no)
* `paid` - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)
* `activities` - extra-curricular activities (binary: yes or no)
* `nursery` - attended nursery school (binary: yes or no)
* `higher` - wants to take higher education (binary: yes or no)
* `internet` - Internet access at home (binary: yes or no)
* `romantic` - with a romantic relationship (binary: yes or no)
* `famrel` - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)
* `freetime` - free time after school (numeric: from 1 - very low to 5 - very high)
* `goout` - going out with friends (numeric: from 1 - very low to 5 - very high)
* `Dalc` - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)
* `Walc` - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)
* `health` - current health status (numeric: from 1 - very bad to 5 - very good)
* `absences` - number of school absences (numeric: from 0 to 93)
* `G1` - first period grade (numeric: from 0 to 20)
* `G2` - second period grade (numeric: from 0 to 20)
* `G3` - final grade (numeric: from 0 to 20, output target)

## Data Wrangling

Our target in this case is `G3` or `Final Grade`, and from description each variable above, there are 2 parameters that have strong and direct correlation with `G3`, namely `G1` ang `G2`. And because of that, this situation could bias our model later, so we need to remove `G1` and `G2`.

```{r}
ggpairs(grade %>% 
  select(c(G1,G2,G3)))
```


```{r warning=F}
grade <- grade %>%
  mutate_at(vars(-age,-G3,-absences),.funs=funs(factor)) %>%
  select(-c(G1,G2))
```


```{r}
str(grade)
```

Checking for missing values.

```{r}
colSums(is.na(grade))
```

Checking for duplicated data.

```{r}
sum(duplicated(grade))
```



checking for target data distribution.

```{r}
plot(as.factor(grade$G3),col = "lightBlue",xlab="Final Score (G3)",ylab="Quantity")
```

From distribution plot above, there is something seems off with our target data. Apart from the high number of students scoring 0, the distribution is normal as expected. Maybe the value 0 is used in place of null. Or maybe the students who did not appear for the exam, or were not allowed to sit for the exam due to some reason are marked as 0. We cannot be sure. But from the checking for missing values above, there is no null values, so maybe grade 0 does not mean null after all. Or maybe, The values of 0 above have an explanation of the available variables, therefore, let's check it.

```{r echo=FALSE}
grade_compare <- grade %>% 
  mutate("type"=as.factor(ifelse(G3==0,"zero","non_zero")))
```


```{r}
ggplot(data = grade_compare,aes(x=G3,y=failures))+
  geom_jitter(aes(col=type))+
  labs(x="Final Score (G3)",y="Failures")
```

***

```{r}
ggplot(data = grade_compare,aes(x=G3,y=studytime))+
  geom_jitter(aes(col=type))+
  labs(x="Final Score (G3)",y="Study Time")
```

***

```{r}
ggplot(data = grade_compare,aes(x=G3,y=Walc))+
  geom_jitter(aes(col=type))+
  labs(x="Final Score (G3)",y="Weekend Alcohol Consumption")
```

***

```{r}
ggplot(data = grade_compare,aes(x=G3,y=health))+
  geom_jitter(aes(col=type))+
  labs(x="Final Score (G3)",y="Health")
```


After checking the correlation between 0 and non-zero values with several variables, it turns out that there is no difference, which means that the value of 0 has no explanation for the reasons for the available variables. Because of this, we need to remove zero values so that our model has good performance later.


```{r}
grade <- grade %>%
  filter(G3!=0)
```


## Exploratory Data Analysis

Because there are many variables, we need to look at the correlation of the target variable with several predictors, so that we can know which variables have a significant effect on the target variable.

```{r warning=FALSE}
ggplot(data = grade,aes(x=age,y=G3))+
  geom_violin(aes(fill=sex),alpha=.9)+
  labs(x="Age",y="Final Score (G3)")
```

***

```{r}
ggplot(data = grade,aes(x = studytime,y=G3,fill=studytime))+
  geom_boxplot(show.legend = F)+
  labs(x="Study Time",y="Final Score (G3)")
```

***

```{r}
ggplot(data = grade,aes(x=G3))+
  geom_density(aes(fill=address),alpha=.9)+
  labs(y="Frequency",x="Final Score (G3)")
```

***

```{r}
ggplot(data = grade,aes(x=G3))+
  geom_bar(aes(fill=romantic),alpha=.9)+
  labs(y="Amount",x="Final Score (G3)")
```

***

```{r}
ggplot(data = grade,aes(x = failures,y=G3,fill=failures))+
  geom_boxplot(show.legend = F)+
  labs(x="Failures",y="Final Score (G3)")
```

***

```{r}
ggplot(data = grade,aes(x = schoolsup,y=G3,fill=schoolsup))+
  geom_boxplot(show.legend = F)+
  labs(x="Extra Educational Support",y="Final Score (G3)")
```

***

```{r}
ggplot(data = grade,aes(x=G3))+
  geom_density(aes(fill=internet),alpha=.9)+
  labs(y="Frequency",x="Final Score (G3)")
```

***

```{r}
ggplot(data = grade,aes(x = freetime,y=G3,fill=freetime))+
  geom_boxplot(show.legend = F)+
  labs(x="Free Time",y="Final Score (G3)")
```

***

```{r}
ggplot(data = grade,aes(x=G3))+
  geom_density(aes(fill=higher),alpha=.9)+
  labs(y="Frequency",x="Final Score (G3)")

```

***

```{r}
ggplot(data = grade,aes(y = G3,x = absences,size=absences,col=absences))+
  geom_jitter(show.legend = F)+
  labs(x="Number of Absences",y="Final Score (G3)")
```
 

## Cross Validation

Before we make the model, we need to split the data into train dataset and test dataset. We will use the train dataset to train the linear regression model. The test dataset will be used as a comparasion and see the performance of our model. We will 80% of the data as the training data and the rest of it as the testing data.

```{r warning=F}
RNGkind(sample.kind = "Rounding")
set.seed(23)

intrain <- sample(nrow(grade),nrow(grade)*.8)

grade_train <- grade[intrain,]
grade_test <- grade[-intrain,]
```

## Modelling

We will try to create several models the linear regression using `G3` as the target value. The models that we will create come from several ways, some from the my understanding or estimation and from stepwise selection.

```{r}
model_grade_all <- lm(formula = G3~.,data = grade_train)

model_grade_none <- lm(formula = G3~1,data = grade_train)

model_grade_selected <- lm(formula = G3~failures+studytime+higher+schoolsup+internet+goout+romantic,data = grade_train)

model_grade_selected2 <- lm(formula = G3~Mjob+Fjob+studytime+failures+schoolsup+paid+health+absences,data = grade_train)

model_grade_backward <- step(object = model_grade_all,direction = "backward",trace = F)

model_both_forward <- step(object = model_grade_none,direction = "both",scope = list(lower=model_grade_none,upper=model_grade_all),trace = F)
```

After making several models, now let's compare each other.

```{r}
performance::compare_performance(model_grade_all,model_grade_backward,model_both_forward,model_grade_selected,model_grade_selected2)
```

From the result above, we will use `model_grade_backward` as our model.

```{r}
summary(model_grade_backward)
```

`model_grade_backward`'s summary above contains lot of information, like predictors that used for making model, five-number summary of residuals, significance level of each predictor (`Pr(>|t|)`) , and R-squared. From `Pr(>|t|)` above, we can get information on which predictors have a significant influence on the target, if the value is below 0.05 (alpha), we asume that the variable has significant effect toward the model, and then the smaller the `Pr(>|t|)` value, the more significant the predictors have on the target, and to make it easier, there is a star symbol which indicates the more stars the more significant the predictor's influence on the target.

## Prediction

After choosing the best model for our dataset, then we need to test our model performance using testing dataset that we have splitted above.

```{r}
grade_test$G3_predicted <- round(predict(object = model_grade_backward,newdata = grade_test),2)
```


```{r,echo=FALSE, message=F}
comparison_result <- grade_test %>% 
  select(c(G3,G3_predicted))
rmarkdown::paged_table(comparison_result, options = list(rows.print = 10))
```


## Evaluation

### Model Performance

From testing performance using testing dataset above, we can evaluate our model using `RMSE`. Root Mean Square Error `RMSE` is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data–how close the observed data points are to the model's predicted value.

```{r}
RMSE(y_pred = predict(model_grade_backward,grade_test),y_true = grade_test$G3)
```


## Assumptions

Assumptions are essentially conditions that should be met before we draw inferences regarding the model estimates. Assumption tests are needed to prove that the resulting model is not misleading, or has biased estimators

#### Normality Test

The linear regression analysis requires all variables to be multivariate normal. This assumption can best be checked with a histogram or a Q-Q-Plot. Normality can be checked with a goodness of fit test, and this time we will use the Shapiro-Wilk normality test.



```{r}
hist(model_grade_backward$residuals,xlab = "Residuals",col = "lightBlue",main = "Residual Distribution Plot")
```


```{r}
qqnorm(model_grade_backward$residuals)
qqline(model_grade_backward$residuals,col="red")
```

Saphiro-Wilk normality test.

```{r}
shapiro.test(model_grade_backward$residuals)
```

The null hypothesis is that the residuals follow normal distribution. With p-value < 0.05, we can conclude that our hypothesis has failed to be rejected, so, our residuals are following the normal distribution.


#### Homoscedasticity Test

Homoscedasticity refers to whether these residuals are equally distributed, or whether they tend to bunch together at some values, and at other values, spread far apart.


```{r}
plot(model_grade_backward$fitted.values,model_grade_backward$residuals,pch=16,col = "black",xlab = "Fitted Values", ylab="Residuals",main = "Residual Plot")
abline(h=0,col="red")
```

Studentized Breusch-Pagan test

```{r warning=F,message=F}
library(lmtest)
bptest(model_grade_backward)
```

The null hypothesis is that the residuals are homoscedastic. With p-value < 0.05, we can conclude that our hypothesis has failed to be rejected, so, our residuals are equally distributed.

#### Autocorrelation Test

Linear regression assumes that there is little or no multicollinearity in the data. Multicollinearity occurs when the independent variables are too highly correlated with each other.

This time, Multicollinearity will be tested with Variance Inflation Factor (VIF). Variance inflation factor of the linear regression is defined as VIF = 1/Tolerance (T). With VIF > 10 there is multicollinearity among the variables.

```{r}
vif(mod = model_grade_backward)
```

All of our predictors that used for making model have VIF < 10. It means that multicolinearity is not present in our model.

## Conclusion

After conducting the evaluation test using RMSE Test, our model (`model_grade_backward` ) has good performance to predict student grade, and besides, our model has passed asumption tests. But, there is One thing to note, however, is that our model has a low r-squared value. The R-square is a measure of explanatory power. With an r-square value of 0.28, it means that our model can only explain 28% of the total data, but this situation does not only happen in our model, even if all predictors are used, the r-square value also ranges in the same number. This is probably because the predictors in our dataset are predominantly dummy variables. But actually that's not entirely bad, because our model uses predictors of high significance, so our model explains how changes in response values are associated with changes in predictor values.
